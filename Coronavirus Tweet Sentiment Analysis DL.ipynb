{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875eeb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To just ignore all the warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a537831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\tusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Basic Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# For basic Ploting graph and charts\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# For more plotting options\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# For machine learning modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "import unidecode\n",
    "import io # allows us to manage the file-related input and output operations\n",
    "import re # regular expression\n",
    "import time\n",
    "import math\n",
    "import string # create a string template for simpler string substitutions\n",
    "import urllib #Python module for fetching URLs\n",
    "import requests # The requests module allows you to send HTTP requests using Python\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f530805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diamentions: 41157 x 6\n"
     ]
    }
   ],
   "source": [
    "# Loading up the data into a DataFrame\n",
    "df = pd.read_csv('Coronavirus Tweets.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Shape of data.\n",
    "print(f'Diamentions: {df.shape[0]} x {df.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0204cd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3804</td>\n",
       "      <td>48756</td>\n",
       "      <td>ÃT: 36.319708,-82.363649</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>As news of the regionÂs first confirmed COVID...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3805</td>\n",
       "      <td>48757</td>\n",
       "      <td>35.926541,-78.753267</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Cashier at grocery store was sharing his insig...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3806</td>\n",
       "      <td>48758</td>\n",
       "      <td>Austria</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Was at the supermarket today. Didn't buy toile...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3807</td>\n",
       "      <td>48759</td>\n",
       "      <td>Atlanta, GA USA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Due to COVID-19 our retail store and classroom...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3808</td>\n",
       "      <td>48760</td>\n",
       "      <td>BHAVNAGAR,GUJRAT</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>For corona prevention,we should stop to buy th...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3809</td>\n",
       "      <td>48761</td>\n",
       "      <td>Makati, Manila</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>All month there hasn't been crowding in the su...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3810</td>\n",
       "      <td>48762</td>\n",
       "      <td>Pitt Meadows, BC, Canada</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Due to the Covid-19 situation, we have increas...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3811</td>\n",
       "      <td>48763</td>\n",
       "      <td>Horningsea</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>#horningsea is a caring community. LetÂs ALL ...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3812</td>\n",
       "      <td>48764</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me: I don't need to stock up on food, I'll jus...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3813</td>\n",
       "      <td>48765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>ADARA Releases COVID-19 Resource Center for Tr...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3814</td>\n",
       "      <td>48766</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Lines at the grocery store have been unpredict...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3815</td>\n",
       "      <td>48767</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>????? ????? ????? ????? ??\\r\\r\\n?????? ????? ?...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3816</td>\n",
       "      <td>48768</td>\n",
       "      <td>Ontario, Canada</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@eyeonthearctic 16MAR20 Russia consumer survei...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3817</td>\n",
       "      <td>48769</td>\n",
       "      <td>North America</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Amazon Glitch Stymies Whole Foods, Fresh Groce...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3818</td>\n",
       "      <td>48770</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>For those who aren't struggling, please consid...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    UserName  ScreenName                   Location     TweetAt   \n",
       "0       3799       48751                     London  16-03-2020  \\\n",
       "1       3800       48752                         UK  16-03-2020   \n",
       "2       3801       48753                  Vagabonds  16-03-2020   \n",
       "3       3802       48754                        NaN  16-03-2020   \n",
       "4       3803       48755                        NaN  16-03-2020   \n",
       "5       3804       48756  ÃT: 36.319708,-82.363649  16-03-2020   \n",
       "6       3805       48757       35.926541,-78.753267  16-03-2020   \n",
       "7       3806       48758                    Austria  16-03-2020   \n",
       "8       3807       48759            Atlanta, GA USA  16-03-2020   \n",
       "9       3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   \n",
       "10      3809       48761             Makati, Manila  16-03-2020   \n",
       "11      3810       48762  Pitt Meadows, BC, Canada   16-03-2020   \n",
       "12      3811       48763                 Horningsea  16-03-2020   \n",
       "13      3812       48764                Chicago, IL  16-03-2020   \n",
       "14      3813       48765                        NaN  16-03-2020   \n",
       "15      3814       48766             Houston, Texas  16-03-2020   \n",
       "16      3815       48767               Saudi Arabia  16-03-2020   \n",
       "17      3816       48768            Ontario, Canada  16-03-2020   \n",
       "18      3817       48769              North America  16-03-2020   \n",
       "19      3818       48770                 Denver, CO  16-03-2020   \n",
       "\n",
       "                                        OriginalTweet           Sentiment  \n",
       "0   @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1   advice Talk to your neighbours family to excha...            Positive  \n",
       "2   Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3   My food stock is not the only one which is emp...            Positive  \n",
       "4   Me, ready to go at supermarket during the #COV...  Extremely Negative  \n",
       "5   As news of the regionÂs first confirmed COVID...            Positive  \n",
       "6   Cashier at grocery store was sharing his insig...            Positive  \n",
       "7   Was at the supermarket today. Didn't buy toile...             Neutral  \n",
       "8   Due to COVID-19 our retail store and classroom...            Positive  \n",
       "9   For corona prevention,we should stop to buy th...            Negative  \n",
       "10  All month there hasn't been crowding in the su...             Neutral  \n",
       "11  Due to the Covid-19 situation, we have increas...  Extremely Positive  \n",
       "12  #horningsea is a caring community. LetÂs ALL ...  Extremely Positive  \n",
       "13  Me: I don't need to stock up on food, I'll jus...            Positive  \n",
       "14  ADARA Releases COVID-19 Resource Center for Tr...            Positive  \n",
       "15  Lines at the grocery store have been unpredict...            Positive  \n",
       "16  ????? ????? ????? ????? ??\\r\\r\\n?????? ????? ?...             Neutral  \n",
       "17  @eyeonthearctic 16MAR20 Russia consumer survei...             Neutral  \n",
       "18  Amazon Glitch Stymies Whole Foods, Fresh Groce...  Extremely Positive  \n",
       "19  For those who aren't struggling, please consid...            Positive  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8437032b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet',\n",
       "       'Sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Names Of columns in our dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "871da49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "    \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\"how'll\": \"how will\",\"how's\": \"how is\",\n",
    "    \"i'd\": \"I would\", \"i'll\": \"I will\",\"i'm\": \"I am\",\"i've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\"it'd\": \"it would\",\"it'll\": \"it will\",\"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\"mightn't\": \"might not\",\"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\"needn't\": \"need not\",\"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\"she's\": \"she is\",\"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\"that'd\": \"that would\",\"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\"there's\": \"there is\",\"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\"they're\": \"they are\",\"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\"what're\": \"what are\",\"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\"where'd\": \"where did\",\"where's\": \"where is\",\n",
    "    \"who'll\": \"who will\",\"who's\": \"who is\",\"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\"you'd\": \"you would\",\"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\"\n",
    "    }\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "\n",
    "    # Remove punctuations\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Contraction mapping\n",
    "    for word in text.split():\n",
    "        if word.lower() in contraction_mapping:\n",
    "            text = text.replace(word, contraction_mapping[word.lower()])\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "    # Remove accented characters\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    # Remove HTML\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f357904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['OriginalTweet'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98f21a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Initialize the spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Initialize the Tweet tokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "# Define the stop words list\n",
    "stop_words = set(stopwords.words('english')) - {'not', 'no', 'but', 'isn\\'t', 'aren\\'t', 'wasn\\'t', 'weren\\'t', 'don', \"don\\'t\", 'should', \"should\\'ve\", 'won', \"won\\'t\"}\n",
    "\n",
    "\n",
    "# Function for tokenization\n",
    "def tokenize(text):\n",
    "    return tweet_tokenizer.tokenize(text)\n",
    "\n",
    "# Function for stop words removal\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Function for getting the wordnet object value corresponding to the POS tag\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Function for lemmatization\n",
    "def lemmatize(tokens):\n",
    "    pos_tagged_text = nltk.pos_tag(tokens)\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag)) for word, pos_tag in pos_tagged_text]\n",
    "\n",
    "# Function for spell correction\n",
    "def spell_correction(tokens):\n",
    "    # Find the words that may be misspelled\n",
    "    misspelled = spell.unknown(tokens)\n",
    "    corrected_text = []\n",
    "    for word in tokens:\n",
    "        # If the word is misspelled then replace it with the most likely word, else leave it as is\n",
    "        if word in misspelled:\n",
    "            corrected_word = spell.correction(word)\n",
    "            # Only append the corrected word if it's not None\n",
    "            if corrected_word is not None:\n",
    "                corrected_text.append(corrected_word)\n",
    "            else:\n",
    "                corrected_text.append(word)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "# Function to apply all the functions\n",
    "def text_preprocessing(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = spell_correction(tokens)\n",
    "    tokens = lemmatize(tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "df['processed_text'] = df['clean_text'].apply(text_preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Sentiment' into numerical values\n",
    "# There are 5 categories of sentiments in the section, let's convert them to 3 main categories\n",
    "df['Sentiment'] = df['Sentiment'].map({'Extremely Negative': 0, 'Negative': 0,'Neutral': 1,'Positive': 2,'Extremely Positive': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9ccfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('spellcorrection.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafec940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee3ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec(x):\n",
    "    doc = nlp(x)\n",
    "    vec = doc.vector\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b03957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vec'] = df['clean_text'].apply(lambda x: get_vec(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(df['vec'].to_numpy())\n",
    "y = df['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate the training time\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Make predictions on train and test data\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics for train set\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "\n",
    "    # Calculate metrics for test set\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    classification = classification_report(y_test, test_predictions)\n",
    "    confusion = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "    # Display the evaluation metrics\n",
    "    print(\"Train Set Accuracy: {:.2f}%\".format(train_accuracy * 100))\n",
    "    print(\"Test Set Accuracy: {:.2f}%\".format(test_accuracy * 100))\n",
    "    print(\"Training Time: {:.2f} seconds\".format(training_time))\n",
    "\n",
    "    try:\n",
    "        # Format classification report\n",
    "        classification_df = pd.DataFrame.from_dict(classification, orient='index')\n",
    "        classification_df.columns = ['Precision', 'Recall', 'F1-score', 'Support']\n",
    "        classification_df = classification_df.round(2)\n",
    "        print(\"\\nPrecision, Recall, F1-score:\")\n",
    "        print(classification_df)\n",
    "    except AttributeError:\n",
    "        print(\"\\nInvalid classification report format. Please check the 'classification' variable.\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification)\n",
    "\n",
    "    # Format confusion matrix\n",
    "    confusion_df = pd.DataFrame(confusion, columns=np.unique(y_test), index=np.unique(y_test))\n",
    "    confusion_df.index.name = 'Actual'\n",
    "    confusion_df.columns.name = 'Predicted'\n",
    "\n",
    "    # Visualize the evaluation metrics\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    sns.heatmap(confusion_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ddd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3885cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=300))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ecefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f2fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b86e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# When creating your model, initialize the optimizer and define a learning rate\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=300))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Add some callbacks for early stopping and reducing the learning rate when the loss plateaus\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[rlrop, es])\n",
    "\n",
    "# Evaluate your model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2639af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# def build_model(optimizer):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(units = 50, kernel_initializer = 'uniform', activation = 'relu', input_dim = X.shape[1]))\n",
    "#     model.add(Dropout(rate = 0.1))\n",
    "#     model.add(Dense(units = 50, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "#     model.add(Dropout(rate = 0.1))\n",
    "#     model.add(Dense(units = 3, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "#     model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# model = KerasClassifier(build_fn = build_model)\n",
    "\n",
    "# parameters = {'batch_size': [16, 32],\n",
    "#               'epochs': [100],\n",
    "#               'optimizer': ['adam', 'rmsprop']}\n",
    "\n",
    "# grid_search = GridSearchCV(estimator = model,\n",
    "#                            param_grid = parameters,\n",
    "#                            scoring = 'accuracy',\n",
    "#                            cv = 10)\n",
    "\n",
    "# grid_search = grid_search.fit(X_train, y_train)\n",
    "# best_parameters = grid_search.best_params_\n",
    "# best_accuracy = grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6038263",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(X_train).any())\n",
    "print(np.isnan(y_train).any())\n",
    "print(np.unique(y_train))\n",
    "print(np.unique(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Evaluate the model\n",
    "model_evaluation(logreg, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce3c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Apply the model evaluation function\n",
    "model_evaluation(forest, X_train_tfidf, y_train, X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97e4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
